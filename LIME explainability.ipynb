{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Load feature files\n",
    "def load_feature_files(base_path):\n",
    "    \"\"\"Load train, validation, and test CSV files from each feature folder.\"\"\"\n",
    "    #feature_sets = [\"Frequency\", \"Gabor\", \"LBP\", \"HOG\", \"Statistical\"]\n",
    "    feature_sets = [\"Frequency\", \"Gabor\"]\n",
    "    data = {}\n",
    "\n",
    "    for feature in feature_sets:\n",
    "        train_path = os.path.join(base_path, feature, \"train.csv\")\n",
    "        val_path = os.path.join(base_path, feature, \"val.csv\")\n",
    "        test_path = os.path.join(base_path, feature, \"test.csv\")\n",
    "\n",
    "        data[feature] = {\n",
    "            \"train\": pd.read_csv(train_path),\n",
    "            \"val\": pd.read_csv(val_path),\n",
    "            \"test\": pd.read_csv(test_path),\n",
    "        }\n",
    "\n",
    "    return data\n",
    "\n",
    "# Update the path to your dataset\n",
    "base_path = r\"E:\\Abroad period research\\hybrid oranges try code\\testing code on brain tumor dataset\\Features\"\n",
    "data = load_feature_files(base_path)\n",
    "\n",
    "# 2. Combine train and val files, then split features and labels\n",
    "def combine_and_split_features(data):\n",
    "    \"\"\"Combine train and val datasets, and split features and labels.\"\"\"\n",
    "    X_train_val, y_train_val = {}, {}\n",
    "    X_test, y_test = {}, {}\n",
    "\n",
    "    for feature, datasets in data.items():\n",
    "        # Combine train and val datasets\n",
    "        combined_train_val = pd.concat([datasets[\"train\"], datasets[\"val\"]], ignore_index=True)\n",
    "\n",
    "        # Split features and labels\n",
    "        X_train_val[feature] = combined_train_val.iloc[:, :-1]  # All columns except last\n",
    "        y_train_val[feature] = combined_train_val.iloc[:, -1]  # Last column as label\n",
    "        X_test[feature] = datasets[\"test\"].iloc[:, :-1]\n",
    "        y_test[feature] = datasets[\"test\"].iloc[:, -1]\n",
    "\n",
    "    return X_train_val, y_train_val, X_test, y_test\n",
    "\n",
    "X_train_val, y_train_val, X_test, y_test = combine_and_split_features(data)\n",
    "\n",
    "# 3. Combine all feature sets into a single DataFrame\n",
    "def combine_features(X_train, X_test):\n",
    "    \"\"\"Concatenate features from all sets into single training and testing DataFrames.\"\"\"\n",
    "    X_train_combined = pd.concat(X_train.values(), axis=1)\n",
    "    X_test_combined = pd.concat(X_test.values(), axis=1)\n",
    "    return X_train_combined, X_test_combined\n",
    "\n",
    "X_train_combined, X_test_combined = combine_features(X_train_val, X_test)\n",
    "y_train_combined = y_train_val[next(iter(y_train_val.keys()))]\n",
    "y_test_combined = y_test[next(iter(y_test.keys()))]\n",
    "\n",
    "# 4. Hyperparameter tuning for DecisionTreeClassifier\n",
    "def tune_decision_tree(X, y):\n",
    "    \"\"\"Tune hyperparameters of Decision Tree using GridSearchCV.\"\"\"\n",
    "    param_grid = {\n",
    "        \"max_depth\": [5, 10, 15],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 5],\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Decision Tree Parameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Train tuned Decision Tree\n",
    "tuned_tree = tune_decision_tree(X_train_combined, y_train_combined)\n",
    "\n",
    "# Calibrate probabilities for soft voting\n",
    "calibrated_tree = CalibratedClassifierCV(tuned_tree, method=\"sigmoid\")\n",
    "calibrated_tree.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# 5. Define additional classifiers (e.g., Random Forest)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Calibrate Random Forest probabilities\n",
    "calibrated_rf = CalibratedClassifierCV(random_forest, method=\"sigmoid\")\n",
    "calibrated_rf.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# 6. Create a Voting Classifier with calibrated classifiers\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"calibrated_tree\", calibrated_tree),\n",
    "        (\"calibrated_rf\", calibrated_rf),\n",
    "    ],\n",
    "    voting=\"soft\",  # Soft voting for probability-based decisions\n",
    ")\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_clf.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = voting_clf.predict(X_test_combined)\n",
    "print(\"\\nEnhanced Voting Classifier Results:\")\n",
    "print(classification_report(y_test_combined, y_pred, digits=4))\n",
    "\n",
    "# 7. Print the final accuracy\n",
    "final_accuracy = accuracy_score(y_test_combined, y_pred)\n",
    "print(f\"Enhanced Voting Classifier Accuracy: {final_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741d5eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad115037",
   "metadata": {},
   "source": [
    "LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8139e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize LIME Explainer\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_train_combined.values,\n",
    "    training_labels=y_train_combined.values,\n",
    "    mode=\"classification\",\n",
    "    feature_names=X_train_combined.columns.tolist(),\n",
    "    class_names=[f\"Class {cls}\" for cls in np.unique(y_train_combined)],\n",
    "    discretize_continuous=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Select instances for explanation (e.g., first 2 instances from the test set)\n",
    "num_samples_to_explain = 2\n",
    "instances_to_explain = X_test_combined.iloc[:num_samples_to_explain]\n",
    "\n",
    "# Explain each selected instance\n",
    "for idx, instance in instances_to_explain.iterrows():\n",
    "    print(f\"\\nExplaining instance {idx}...\")\n",
    "\n",
    "    # Generate explanation for the instance\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        data_row=instance.values,\n",
    "        predict_fn=voting_clf.predict_proba,\n",
    "        num_features=10,  # Number of top features to display in the explanation\n",
    "    )\n",
    "\n",
    "    # Save explanation as an HTML file\n",
    "    html_filename = f\"lime_explanation_instance_{idx}.html\"\n",
    "    explanation.save_to_file(html_filename)\n",
    "    print(f\"LIME explanation for instance {idx} saved as '{html_filename}'\")\n",
    "\n",
    "    # Optional: Display the explanation in the console\n",
    "    explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557854b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7672946",
   "metadata": {},
   "source": [
    "Saving LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# 8. Perform Class-Specific Feature Selection and Evaluation\n",
    "def perform_class_specific_feature_selection(X_train, y_train, X_test, y_test, num_features=5):\n",
    "    \"\"\"\n",
    "    Perform one-vs-all feature selection and train a Decision Tree for each class.\n",
    "    \"\"\"\n",
    "    class_specific_features = {}\n",
    "    class_classifiers = {}\n",
    "    evaluation_results = {}\n",
    "    lime_explainers = {}\n",
    "    \n",
    "    # Get the unique classes from the training labels\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    for class_label in classes:\n",
    "        print(f\"\\nPerforming feature selection and training for class {class_label}...\")\n",
    "\n",
    "        # One-vs-all approach (binary labels: 1 for current class, 0 for others)\n",
    "        y_binary_train = y_train.apply(lambda x: 1 if x == class_label else 0)\n",
    "        y_binary_test = y_test.apply(lambda x: 1 if x == class_label else 0)\n",
    "        \n",
    "        # Perform mutual information-based feature selection for this class\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n",
    "        selector.fit(X_train, y_binary_train)\n",
    "        \n",
    "        # Get selected feature indices and names\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features = X_train.columns[selected_indices]\n",
    "        \n",
    "        # Store class-specific features\n",
    "        class_specific_features[class_label] = selected_features\n",
    "        print(f\"Selected features for class {class_label}: {list(selected_features)}\")\n",
    "        \n",
    "        # Train a Decision Tree Classifier using only the selected features for this class\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_test_selected = X_test[selected_features]\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        clf.fit(X_train_selected, y_binary_train)\n",
    "        \n",
    "        # Store the classifier for this class\n",
    "        class_classifiers[class_label] = clf\n",
    "        \n",
    "        # Evaluate the classifier on test data\n",
    "        y_pred_test = clf.predict(X_test_selected)\n",
    "        accuracy = accuracy_score(y_binary_test, y_pred_test)\n",
    "        report = classification_report(y_binary_test, y_pred_test, digits=4)\n",
    "        \n",
    "        # Store evaluation results\n",
    "        evaluation_results[class_label] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy for class {class_label}: {accuracy:.4f}\")\n",
    "        print(f\"Classification Report for class {class_label}:\\n{report}\")\n",
    "        \n",
    "        # Initialize LIME Explainer for the current class\n",
    "        lime_explainer = LimeTabularExplainer(\n",
    "            training_data=X_train_selected.values,\n",
    "            training_labels=y_binary_train,\n",
    "            mode='classification',\n",
    "            feature_names=selected_features.tolist(),\n",
    "            class_names=[f'Not {class_label}', f'{class_label}'],\n",
    "            discretize_continuous=True\n",
    "        )\n",
    "        lime_explainers[class_label] = lime_explainer\n",
    "        \n",
    "    return class_specific_features, class_classifiers, evaluation_results, lime_explainers\n",
    "\n",
    "\n",
    "# Prepare the inputs for feature selection\n",
    "# Combine training and validation sets for feature selection\n",
    "X_train_combined, X_test_combined = combine_features(X_train_val, X_test)\n",
    "y_train_combined = y_train_val[next(iter(y_train_val.keys()))]\n",
    "y_test_combined = y_test[next(iter(y_test.keys()))]\n",
    "\n",
    "# Call the function for class-specific feature selection and evaluation\n",
    "class_specific_features, class_classifiers, evaluation_results, lime_explainers = perform_class_specific_feature_selection(\n",
    "    X_train_combined, y_train_combined, X_test_combined, y_test_combined, num_features=5\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nClass-Specific Feature Selection Results:\")\n",
    "for class_label, results in evaluation_results.items():\n",
    "    print(f\"\\nClass {class_label} Results:\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Classification Report:\\n{results['classification_report']}\")\n",
    "\n",
    "# Example: Explaining predictions for 2 sample instances from each class (from X_test_combined)\n",
    "for class_label in class_specific_features.keys():\n",
    "    print(f\"\\nExplaining predictions for class {class_label}...\")\n",
    "    \n",
    "    # Get the classifier and selected features for the current class\n",
    "    selected_features = class_specific_features[class_label]  # Get selected features for the current class\n",
    "    lime_explainer = lime_explainers[class_label]\n",
    "\n",
    "    # Get 2 samples from the test set for the current class\n",
    "    class_samples = X_test_combined[y_test_combined == class_label].iloc[:2]  # Get first 2 samples for this class\n",
    "\n",
    "    for sample_index, sample_instance in class_samples.iterrows():\n",
    "        # Make a prediction for the chosen instance using the model for the current class\n",
    "        predicted_class = class_classifiers[class_label].predict([sample_instance[selected_features].values])[0]\n",
    "\n",
    "        # Explain the prediction for this instance\n",
    "        explanation = lime_explainer.explain_instance(\n",
    "            sample_instance[selected_features].values, \n",
    "            class_classifiers[class_label].predict_proba, \n",
    "            num_features=5\n",
    "        )\n",
    "\n",
    "        # Save the explanation as an HTML file for each sample (with UTF-8 encoding)\n",
    "        html_output = explanation.as_html()\n",
    "\n",
    "        # Inject custom CSS to remove scrollbars\n",
    "        html_output = html_output.replace(\n",
    "            \"<head>\",\n",
    "            \"\"\"<head><style>body { overflow: hidden; }</style>\"\"\"\n",
    "        )\n",
    "\n",
    "        html_filename = f\"lime_explanation_class_{class_label}_sample_{sample_index}.html\"\n",
    "        \n",
    "        # Use UTF-8 encoding to avoid UnicodeEncodeError\n",
    "        with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_output)\n",
    "\n",
    "        print(f\"LIME explanation for sample {sample_index} of class {class_label} saved as '{html_filename}'\")\n",
    "        \n",
    "        # Print Intercept, Local Prediction, and Right (if available)\n",
    "        print(\"\\nAdditional LIME Details:\")\n",
    "        print(f\"Intercept: {explanation.intercept}\")\n",
    "        print(f\"Local Prediction: {explanation.local_pred}\")\n",
    "        print(f\"Right: {explanation.score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f77b6d",
   "metadata": {},
   "source": [
    "Saving Explaination as HTML page with Additional LIME Details such as \n",
    "Intercept, Local Prediction, and Right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53949a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# 8. Perform Class-Specific Feature Selection and Evaluation\n",
    "def perform_class_specific_feature_selection(X_train, y_train, X_test, y_test, num_features=5):\n",
    "    \"\"\"\n",
    "    Perform one-vs-all feature selection and train a Decision Tree for each class.\n",
    "    \"\"\"\n",
    "    class_specific_features = {}\n",
    "    class_classifiers = {}\n",
    "    evaluation_results = {}\n",
    "    lime_explainers = {}\n",
    "    \n",
    "    # Get the unique classes from the training labels\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    for class_label in classes:\n",
    "        print(f\"\\nPerforming feature selection and training for class {class_label}...\")\n",
    "\n",
    "        # One-vs-all approach (binary labels: 1 for current class, 0 for others)\n",
    "        y_binary_train = y_train.apply(lambda x: 1 if x == class_label else 0)\n",
    "        y_binary_test = y_test.apply(lambda x: 1 if x == class_label else 0)\n",
    "        \n",
    "        # Perform mutual information-based feature selection for this class\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n",
    "        selector.fit(X_train, y_binary_train)\n",
    "        \n",
    "        # Get selected feature indices and names\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features = X_train.columns[selected_indices]\n",
    "        \n",
    "        # Store class-specific features\n",
    "        class_specific_features[class_label] = selected_features\n",
    "        print(f\"Selected features for class {class_label}: {list(selected_features)}\")\n",
    "        \n",
    "        # Train a Decision Tree Classifier using only the selected features for this class\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_test_selected = X_test[selected_features]\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        clf.fit(X_train_selected, y_binary_train)\n",
    "        \n",
    "        # Store the classifier for this class\n",
    "        class_classifiers[class_label] = clf\n",
    "        \n",
    "        # Evaluate the classifier on test data\n",
    "        y_pred_test = clf.predict(X_test_selected)\n",
    "        accuracy = accuracy_score(y_binary_test, y_pred_test)\n",
    "        report = classification_report(y_binary_test, y_pred_test, digits=4)\n",
    "        \n",
    "        # Store evaluation results\n",
    "        evaluation_results[class_label] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy for class {class_label}: {accuracy:.4f}\")\n",
    "        print(f\"Classification Report for class {class_label}:\\n{report}\")\n",
    "        \n",
    "        # Initialize LIME Explainer for the current class\n",
    "        lime_explainer = LimeTabularExplainer(\n",
    "            training_data=X_train_selected.values,\n",
    "            training_labels=y_binary_train,\n",
    "            mode='classification',\n",
    "            feature_names=selected_features.tolist(),\n",
    "            class_names=[f'Not {class_label}', f'{class_label}'],\n",
    "            discretize_continuous=True\n",
    "        )\n",
    "        lime_explainers[class_label] = lime_explainer\n",
    "        \n",
    "    return class_specific_features, class_classifiers, evaluation_results, lime_explainers\n",
    "\n",
    "\n",
    "# Prepare the inputs for feature selection\n",
    "# Combine training and validation sets for feature selection\n",
    "X_train_combined, X_test_combined = combine_features(X_train_val, X_test)\n",
    "y_train_combined = y_train_val[next(iter(y_train_val.keys()))]\n",
    "y_test_combined = y_test[next(iter(y_test.keys()))]\n",
    "\n",
    "# Call the function for class-specific feature selection and evaluation\n",
    "class_specific_features, class_classifiers, evaluation_results, lime_explainers = perform_class_specific_feature_selection(\n",
    "    X_train_combined, y_train_combined, X_test_combined, y_test_combined, num_features=5\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nClass-Specific Feature Selection Results:\")\n",
    "for class_label, results in evaluation_results.items():\n",
    "    print(f\"\\nClass {class_label} Results:\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Classification Report:\\n{results['classification_report']}\")\n",
    "\n",
    "# Example: Explaining predictions for 2 sample instances from each class (from X_test_combined)\n",
    "for class_label in class_specific_features.keys():\n",
    "    print(f\"\\nExplaining predictions for class {class_label}...\")\n",
    "    \n",
    "    # Get the classifier and selected features for the current class\n",
    "    selected_features = class_specific_features[class_label]  # Get selected features for the current class\n",
    "    lime_explainer = lime_explainers[class_label]\n",
    "\n",
    "    # Get 2 samples from the test set for the current class\n",
    "    class_samples = X_test_combined[y_test_combined == class_label].iloc[:2]  # Get first 2 samples for this class\n",
    "\n",
    "    for sample_index, sample_instance in class_samples.iterrows():\n",
    "        # Make a prediction for the chosen instance using the model for the current class\n",
    "        predicted_class = class_classifiers[class_label].predict([sample_instance[selected_features].values])[0]\n",
    "\n",
    "        # Explain the prediction for this instance\n",
    "        explanation = lime_explainer.explain_instance(\n",
    "            sample_instance[selected_features].values, \n",
    "            class_classifiers[class_label].predict_proba, \n",
    "            num_features=5\n",
    "        )\n",
    "\n",
    "        # Append additional LIME details (Intercept, Local Prediction, Right score)\n",
    "        additional_details = f\"\"\"\n",
    "        <br><strong>Intercept:</strong> {explanation.intercept}<br>\n",
    "        <strong>Local Prediction:</strong> {explanation.local_pred}<br>\n",
    "        <strong>Right:</strong> {explanation.score}<br>\n",
    "        \"\"\"\n",
    "\n",
    "        # Add the additional details to the explanation HTML output\n",
    "        html_output = explanation.as_html() + additional_details\n",
    "        \n",
    "        # Save the explanation as an HTML file for each sample (with UTF-8 encoding)\n",
    "        html_filename = f\"lime_explanation_class_{class_label}_sample_{sample_index}.html\"\n",
    "        \n",
    "        # Use UTF-8 encoding to avoid UnicodeEncodeError\n",
    "        with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_output)\n",
    "\n",
    "        print(f\"LIME explanation for sample {sample_index} of class {class_label} saved as '{html_filename}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c7ccc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f737764",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deadad88",
   "metadata": {},
   "source": [
    "Final LIME code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# 8. Perform Class-Specific Feature Selection and Evaluation\n",
    "def perform_class_specific_feature_selection(X_train, y_train, X_test, y_test, num_features=5):\n",
    "    \"\"\"\n",
    "    Perform one-vs-all feature selection and train a Decision Tree for each class.\n",
    "    \"\"\"\n",
    "    class_specific_features = {}\n",
    "    class_classifiers = {}\n",
    "    evaluation_results = {}\n",
    "    lime_explainers = {}\n",
    "    \n",
    "    # Get the unique classes from the training labels\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    for class_label in classes:\n",
    "        print(f\"\\nPerforming feature selection and training for class {class_label}...\")\n",
    "\n",
    "        # One-vs-all approach (binary labels: 1 for current class, 0 for others)\n",
    "        y_binary_train = y_train.apply(lambda x: 1 if x == class_label else 0)\n",
    "        y_binary_test = y_test.apply(lambda x: 1 if x == class_label else 0)\n",
    "        \n",
    "        # Perform mutual information-based feature selection for this class\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n",
    "        selector.fit(X_train, y_binary_train)\n",
    "        \n",
    "        # Get selected feature indices and names\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features = X_train.columns[selected_indices]\n",
    "        \n",
    "        # Store class-specific features\n",
    "        class_specific_features[class_label] = selected_features\n",
    "        print(f\"Selected features for class {class_label}: {list(selected_features)}\")\n",
    "        \n",
    "        # Train a Decision Tree Classifier using only the selected features for this class\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_test_selected = X_test[selected_features]\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        clf.fit(X_train_selected, y_binary_train)\n",
    "        \n",
    "        # Store the classifier for this class\n",
    "        class_classifiers[class_label] = clf\n",
    "        \n",
    "        # Evaluate the classifier on test data\n",
    "        y_pred_test = clf.predict(X_test_selected)\n",
    "        accuracy = accuracy_score(y_binary_test, y_pred_test)\n",
    "        report = classification_report(y_binary_test, y_pred_test, digits=4)\n",
    "        \n",
    "        # Store evaluation results\n",
    "        evaluation_results[class_label] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy for class {class_label}: {accuracy:.4f}\")\n",
    "        print(f\"Classification Report for class {class_label}:\\n{report}\")\n",
    "        \n",
    "        # Initialize LIME Explainer for the current class\n",
    "        lime_explainer = LimeTabularExplainer(\n",
    "            training_data=X_train_selected.values,\n",
    "            training_labels=y_binary_train,\n",
    "            mode='classification',\n",
    "            feature_names=selected_features.tolist(),\n",
    "            # FIX: use Class 0 / Class 1\n",
    "            class_names=['Class 0', 'Class 1'],\n",
    "            discretize_continuous=True\n",
    "        )\n",
    "        lime_explainers[class_label] = lime_explainer\n",
    "        \n",
    "    return class_specific_features, class_classifiers, evaluation_results, lime_explainers\n",
    "\n",
    "\n",
    "# === Prepare the inputs for feature selection ===\n",
    "# NOTE: Ensure combine_features(), X_train_val, X_test, y_train_val, y_test are defined earlier\n",
    "X_train_combined, X_test_combined = combine_features(X_train_val, X_test)\n",
    "y_train_combined = y_train_val[next(iter(y_train_val.keys()))]\n",
    "y_test_combined = y_test[next(iter(y_test.keys()))]\n",
    "\n",
    "# Call the function for class-specific feature selection and evaluation\n",
    "class_specific_features, class_classifiers, evaluation_results, lime_explainers = perform_class_specific_feature_selection(\n",
    "    X_train_combined, y_train_combined, X_test_combined, y_test_combined, num_features=5\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nClass-Specific Feature Selection Results:\")\n",
    "for class_label, results in evaluation_results.items():\n",
    "    print(f\"\\nClass {class_label} Results:\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Classification Report:\\n{results['classification_report']}\")\n",
    "\n",
    "# === Example: Explaining predictions for 2 sample instances from each class ===\n",
    "for class_label in class_specific_features.keys():\n",
    "    print(f\"\\nExplaining predictions for class {class_label}...\")\n",
    "    \n",
    "    # Get the classifier and selected features for the current class\n",
    "    selected_features = class_specific_features[class_label]\n",
    "    lime_explainer = lime_explainers[class_label]\n",
    "\n",
    "    # Get 2 samples from the test set for the current class\n",
    "    class_samples = X_test_combined[y_test_combined == class_label].iloc[:2]\n",
    "\n",
    "    for sample_index, sample_instance in class_samples.iterrows():\n",
    "        # Make a prediction for the chosen instance\n",
    "        predicted_class = class_classifiers[class_label].predict([sample_instance[selected_features].values])[0]\n",
    "\n",
    "        # Explain the prediction for this instance\n",
    "        explanation = lime_explainer.explain_instance(\n",
    "            sample_instance[selected_features].values, \n",
    "            class_classifiers[class_label].predict_proba, \n",
    "            num_features=5\n",
    "        )\n",
    "\n",
    "        # ✅ Keep the colorful HTML from as_html()\n",
    "        html_output = explanation.as_html()\n",
    "\n",
    "        # ✅ Add extra details below the chart, separated with spacing\n",
    "        additional_details = f\"\"\"\n",
    "        <br><hr>\n",
    "        <strong>Predicted Class:</strong> Class {predicted_class}<br>\n",
    "        <strong>Intercept:</strong> {explanation.intercept}<br>\n",
    "        <strong>Local Prediction:</strong> {explanation.local_pred}<br>\n",
    "        <strong>Score:</strong> {explanation.score}<br>\n",
    "        \"\"\"\n",
    "\n",
    "        # Final HTML with graph + extra info\n",
    "        html_output += additional_details\n",
    "        \n",
    "        # Save the explanation as an HTML file\n",
    "        html_filename = f\"lime_explanation_class_{class_label}_sample_{sample_index}.html\"\n",
    "        with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_output)\n",
    "\n",
    "        print(f\"LIME explanation for sample {sample_index} of class {class_label} saved as '{html_filename}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

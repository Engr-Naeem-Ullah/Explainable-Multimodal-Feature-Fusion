{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Statistical Features from Training, Validation, and Testing Extracted Deep Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, entropy, kurtosis, variation, iqr\n",
    "import os\n",
    "\n",
    "# Load extracted features\n",
    "feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\extracted_features'\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "os.makedirs(stat_feature_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(feature_dir, \"train_features.pkl\"), 'rb') as f:\n",
    "    train_features, train_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"val_features.pkl\"), 'rb') as f:\n",
    "    val_features, val_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"test_features.pkl\"), 'rb') as f:\n",
    "    test_features, test_labels = pickle.load(f)\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels, if needed\n",
    "if len(train_labels.shape) > 1 and train_labels.shape[1] > 1:\n",
    "    train_labels = np.argmax(train_labels, axis=1)\n",
    "\n",
    "if len(val_labels.shape) > 1 and val_labels.shape[1] > 1:\n",
    "    val_labels = np.argmax(val_labels, axis=1)\n",
    "\n",
    "if len(test_labels.shape) > 1 and test_labels.shape[1] > 1:\n",
    "    test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Function to calculate signal-to-noise ratio\n",
    "def signal_to_noise(f):\n",
    "    mean = np.mean(f)\n",
    "    std = np.std(f)\n",
    "    return mean / (std + 1e-6)  # Adding small constant to avoid division by zero\n",
    "\n",
    "# Function to calculate more advanced statistical features from deep features\n",
    "def calculate_statistical_features(features):\n",
    "    stats_features = []\n",
    "    for f in features:\n",
    "        stats = {\n",
    "            'mean': np.mean(f),\n",
    "            'std_dev': np.std(f),\n",
    "            'variance': np.var(f),\n",
    "            'median': np.median(f),\n",
    "            'range': np.ptp(f),  # Peak-to-peak range\n",
    "            'skewness': skew(f),\n",
    "            'kurtosis': kurtosis(f),\n",
    "            'entropy': entropy(np.abs(f) + 1e-6),  # Add small constant to avoid log(0)\n",
    "            'energy': np.sum(f ** 2),  # Sum of squared elements\n",
    "            'contrast': np.std(f) ** 2,  # Contrast as variance\n",
    "            'mean_abs_dev': np.mean(np.abs(f - np.mean(f))),\n",
    "            'min_value': np.min(f),\n",
    "            'max_value': np.max(f),\n",
    "            'iqr': iqr(f),  # Interquartile range\n",
    "            'percentile_25': np.percentile(f, 25),\n",
    "            'percentile_50': np.percentile(f, 50),  # Median\n",
    "            'percentile_75': np.percentile(f, 75),\n",
    "            'signal_to_noise': signal_to_noise(f),\n",
    "            'coef_of_var': variation(f),  # Coefficient of variation\n",
    "            'autocorrelation': np.corrcoef(f[:-1], f[1:])[0, 1] if len(f) > 1 else 0,  # Lag-1 autocorrelation\n",
    "            'shannon_entropy': -np.sum(f * np.log2(f + 1e-6)),  # Shannon entropy for diversity measure\n",
    "            'root_mean_square': np.sqrt(np.mean(f ** 2)),  # Root mean square\n",
    "            'harmonic_mean': len(f) / np.sum(1.0 / (f + 1e-6)),  # Harmonic mean\n",
    "            'geometric_mean': np.exp(np.mean(np.log(f + 1e-6))),  # Geometric mean\n",
    "            'std_error_mean': np.std(f) / np.sqrt(len(f)),  # Standard error of the mean\n",
    "            'median_abs_dev': np.median(np.abs(f - np.median(f))),  # Median absolute deviation\n",
    "        }\n",
    "        stats_features.append(stats)\n",
    "    return stats_features\n",
    "\n",
    "# Function to save features and labels as CSV\n",
    "def save_statistical_features_as_csv(features, labels, set_name):\n",
    "    df = pd.DataFrame(features)\n",
    "    df['label'] = labels  # Ensure labels are a 1D array\n",
    "    df.to_csv(os.path.join(stat_feature_dir, f\"{set_name}_stat_features.csv\"), index=False)\n",
    "\n",
    "# Calculate and save statistical features for each dataset\n",
    "for set_name, features, labels in [(\"train\", train_features, train_labels), \n",
    "                                   (\"val\", val_features, val_labels), \n",
    "                                   (\"test\", test_features, test_labels)]:\n",
    "    stats_features = calculate_statistical_features(features)\n",
    "    save_statistical_features_as_csv(stats_features, labels, set_name)\n",
    "\n",
    "print(\"Statistical features calculated and saved in CSV format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Statistical Features and Train & Evaluate Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Rulefit on the statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "# Load the CSV files\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_data.drop(columns=['label']).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "val_features = val_data.drop(columns=['label']).values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "test_features = test_data.drop(columns=['label']).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluating the statistical features (Mutual information and feature importance appraoch)\n",
    "I used Mutual Information and Feature Importance to directly assess how well your statistical features relate to the class labels. If many features have low MI scores or feature importance values close to zero, they may not be contributing to the classification, and we might need to generate new features or revisit the feature extraction method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load the statistical features and labels from CSV\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "\n",
    "# Load the CSV file\n",
    "train_df = pd.read_csv(f\"{stat_feature_dir}/train_stat_features.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "train_features = train_df.drop(columns=['label']).values  # Drop the 'label' column to get features\n",
    "train_labels = train_df['label'].values  # Extract the labels\n",
    "\n",
    "# Names of the statistical features (based on the columns in the CSV)\n",
    "stat_feature_names = train_df.columns[:-1]  # All columns except 'label' are feature names\n",
    "\n",
    "# 1. Mutual Information (MI) for each feature\n",
    "# Mutual Information helps to see which features carry the most information about the target\n",
    "mi_scores = mutual_info_classif(train_features, train_labels, discrete_features=False)\n",
    "mi_scores_df = pd.DataFrame({'Feature': stat_feature_names, 'MI Score': mi_scores})\n",
    "mi_scores_df.sort_values(by='MI Score', ascending=False, inplace=True)\n",
    "\n",
    "# Display MI scores for each feature\n",
    "print(mi_scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features with more than 50% of data 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Function to find features with more than 50% zeros, excluding the label column\n",
    "def find_zero_features(df):\n",
    "    zero_features = [col for col in df.columns if col != 'label' and (df[col] == 0).mean() > 0.5]\n",
    "    return zero_features\n",
    "\n",
    "# Identify features with more than 50% zeros across all datasets\n",
    "train_zero_features = find_zero_features(train_data)\n",
    "val_zero_features = find_zero_features(val_data)\n",
    "test_zero_features = find_zero_features(test_data)\n",
    "\n",
    "# Union of features with more than 50% zeros across all datasets\n",
    "all_zero_features = set(train_zero_features).union(set(val_zero_features)).union(set(test_zero_features))\n",
    "\n",
    "# Display features with more than 50% zeros in any dataset\n",
    "print(f\"Features with more than 50% zeros in any dataset: {list(all_zero_features)}\")\n",
    "\n",
    "# Drop these features from all datasets, keeping the label column\n",
    "train_filtered = train_data.drop(columns=all_zero_features)\n",
    "val_filtered = val_data.drop(columns=all_zero_features)\n",
    "test_filtered = test_data.drop(columns=all_zero_features)\n",
    "\n",
    "# Save the filtered datasets\n",
    "train_filtered.to_csv(os.path.join(output_dir, \"filtered_train_stat_features.csv\"), index=False)\n",
    "val_filtered.to_csv(os.path.join(output_dir, \"filtered_val_stat_features.csv\"), index=False)\n",
    "test_filtered.to_csv(os.path.join(output_dir, \"filtered_test_stat_features.csv\"), index=False)\n",
    "\n",
    "print(\"Filtered datasets with selected features and labels have been saved in the 'filtered_statistical_features' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree evaluation using filtered_statistical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation on filtered statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_data = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_data.drop(columns=['label']).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "val_features = val_data.drop(columns=['label']).values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "test_features = test_data.drop(columns=['label']).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features\\rulefit_rules_on_filtered_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying mutual information based approach to select 18 best best features among filtered_statistical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\18 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 18 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=18)  # Select top 18 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"18_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"18_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"18_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating decision tree using 18 selected statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\18 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Rulefit on 18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\18 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"18_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"18_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"18_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_18_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 15 statistical features based on mutual importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\15 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"15_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"15_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"15_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree evaluation using 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\15 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"15_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"15_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"15_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation using 15 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\15 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"15_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"15_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"15_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_15_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting 12 most important features among filtered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\12 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=12)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"12_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"12_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"12_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating decision tree on 12 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\12 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"12_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"12_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"12_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutaion rulefit on 12 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\12 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"12_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"12_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"12_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_12_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection 9 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\9 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=9)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"9_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"9_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"9_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree evaluation on 9 selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\9 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"9_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"9_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"9_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation using 9 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\9 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"9_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"9_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"9_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_9_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 6 most important features based on mutual importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\6 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=6)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"6_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"6_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"6_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating decision tree on 6 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\6 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"6_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"6_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"6_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Rulefit on 6 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\6 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"6_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"6_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"6_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_df.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = rf.predict(train_val_features)\n",
    "train_predictions_discrete = np.round(train_predictions).astype(int)\n",
    "train_accuracy = accuracy_score(train_val_labels, train_predictions_discrete)\n",
    "\n",
    "print(f\"Combined Training Accuracy: {train_accuracy:.4f}\")\n",
    "train_conf_matrix = confusion_matrix(train_val_labels, train_predictions_discrete)\n",
    "print(\"Combined Training Confusion Matrix:\")\n",
    "print(train_conf_matrix)\n",
    "print(\"Combined Training Classification Report:\")\n",
    "print(classification_report(train_val_labels, train_predictions_discrete, digits=4))\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_6_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 3 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\3 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=3)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"3_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"3_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"3_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree evaluation on 3 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\3 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4, \n",
    "    min_samples_split=2, \n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation using 3 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\3 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"3_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"3_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\rulefit_rules_on_3_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection using accumulated local effects (ALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (if not already installed)\n",
    "# pip install alibi pandas numpy scikit-learn\n",
    "\n",
    "from alibi.explainers import ALE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# # Load dataset\n",
    "# Define the directory containing the CSV files\n",
    "combined_features_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test_stat_features.csv\"))\n",
    "\n",
    "\n",
    "# # # Define the directory containing the CSV files\n",
    "# combined_features_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\3 best features'\n",
    "\n",
    "# # Load the CSV files\n",
    "# train_df = pd.read_csv(os.path.join(combined_features_path, \"3_training_selected_features.csv\"))\n",
    "# val_df = pd.read_csv(os.path.join(combined_features_path, \"3_validation_selected_features.csv\"))\n",
    "# test_df = pd.read_csv(os.path.join(combined_features_path, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "# Combine train and validation sets\n",
    "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "X_train_val = train_val_df.drop(columns=[\"label\"])\n",
    "y_train_val = train_val_df[\"label\"]\n",
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Train Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "dt_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Wrapper for the predictor to return probabilities (required by ALE)\n",
    "def predictor(X):\n",
    "    return dt_model.predict_proba(X)\n",
    "\n",
    "# Apply ALE\n",
    "ale = ALE(predictor, feature_names=X_train_val.columns.tolist())\n",
    "ale_exp = ale.explain(X_train_val.to_numpy())  # Ensure input is a NumPy array\n",
    "\n",
    "# Compute feature variance from ALE\n",
    "ale_variances = [np.var(ale_exp.ale_values[i]) for i in range(len(X_train_val.columns))]\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": X_train_val.columns,\n",
    "    \"ale_variance\": ale_variances\n",
    "}).sort_values(by=\"ale_variance\", ascending=False)\n",
    "\n",
    "# Select features with highest ALE variance\n",
    "top_features = feature_importance[\"feature\"].iloc[:3]  # Top 10 features\n",
    "X_train_val_selected = X_train_val[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "# Retrain model with selected features\n",
    "dt_model_selected = DecisionTreeClassifier(random_state=1)\n",
    "dt_model_selected.fit(X_train_val_selected, y_train_val)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = dt_model_selected.predict(X_test_selected)\n",
    "print(\"Accuracy with selected features:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nTop features based on ALE variance:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined ALE variance and mutual information to ensure robust feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (if not already installed)\n",
    "# pip install alibi pandas numpy scikit-learn\n",
    "\n",
    "from alibi.explainers import ALE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "combined_features_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test_stat_features.csv\"))\n",
    "\n",
    "# Combine train and validation sets\n",
    "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "X_train_val = train_val_df.drop(columns=[\"label\"])\n",
    "y_train_val = train_val_df[\"label\"]\n",
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_val = pd.DataFrame(scaler.fit_transform(X_train_val), columns=X_train_val.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Train Decision Tree for ALE computation\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "dt_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Wrapper for the predictor to return probabilities\n",
    "def predictor(X):\n",
    "    return dt_model.predict_proba(X)\n",
    "\n",
    "# Apply ALE\n",
    "ale = ALE(predictor, feature_names=X_train_val.columns.tolist())\n",
    "ale_exp = ale.explain(X_train_val.to_numpy())  # Ensure input is a NumPy array\n",
    "\n",
    "# Compute ALE variances\n",
    "ale_variances = [np.var(ale_exp.ale_values[i]) for i in range(len(X_train_val.columns))]\n",
    "feature_importance_ale = pd.DataFrame({\n",
    "    \"feature\": X_train_val.columns,\n",
    "    \"ale_variance\": ale_variances\n",
    "}).sort_values(by=\"ale_variance\", ascending=False)\n",
    "\n",
    "# Compute Mutual Information\n",
    "mi_scores = mutual_info_classif(X_train_val, y_train_val)\n",
    "feature_importance_mi = pd.DataFrame({\n",
    "    \"feature\": X_train_val.columns,\n",
    "    \"mutual_info\": mi_scores\n",
    "}).sort_values(by=\"mutual_info\", ascending=False)\n",
    "\n",
    "# Combine ALE and Mutual Information scores\n",
    "combined_scores = feature_importance_ale.merge(\n",
    "    feature_importance_mi, on=\"feature\"\n",
    ").assign(combined_score=lambda df: df[\"ale_variance\"] * df[\"mutual_info\"])\n",
    "\n",
    "# Select top 5 features based on combined score\n",
    "top_features = combined_scores.sort_values(by=\"combined_score\", ascending=False)[\"feature\"].iloc[:3]\n",
    "X_train_val_selected = X_train_val[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=1), param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_val_selected, y_train_val)\n",
    "\n",
    "# Best Decision Tree model\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_dt_model.predict(X_test_selected)\n",
    "print(\"Accuracy with selected features and tuned Decision Tree:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Display top features based on combined score\n",
    "print(\"\\nTop features based on ALE and Mutual Information:\")\n",
    "print(combined_scores.sort_values(by=\"combined_score\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (if not already installed)\n",
    "# pip install alibi pandas numpy scikit-learn\n",
    "\n",
    "from alibi.explainers import ALE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "combined_features_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\statistical_features'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test_stat_features.csv\"))\n",
    "\n",
    "# Combine train and validation sets\n",
    "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "X_train_val = train_val_df.drop(columns=[\"label\"])\n",
    "y_train_val = train_val_df[\"label\"]\n",
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_val_scaled = pd.DataFrame(scaler.fit_transform(X_train_val), columns=X_train_val.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Train Decision Tree for ALE computation\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "dt_model.fit(X_train_val_scaled, y_train_val)\n",
    "\n",
    "# Wrapper for the predictor to return probabilities\n",
    "def predictor(X):\n",
    "    return dt_model.predict_proba(X)\n",
    "\n",
    "# Apply ALE with improved handling\n",
    "ale = ALE(predictor, feature_names=X_train_val.columns.tolist())\n",
    "ale_exp = ale.explain(X_train_val_scaled.to_numpy())  # Ensure input is a NumPy array\n",
    "\n",
    "# Compute ALE variances and smooth variance for stability\n",
    "ale_variances = [np.var(ale_exp.ale_values[i]) for i in range(len(X_train_val_scaled.columns))]\n",
    "ale_smooth_variances = [np.mean(np.gradient(ale_exp.ale_values[i])**2) for i in range(len(X_train_val_scaled.columns))]\n",
    "feature_importance_ale = pd.DataFrame({\n",
    "    \"feature\": X_train_val.columns,\n",
    "    \"ale_variance\": ale_variances,\n",
    "    \"ale_smooth_variance\": ale_smooth_variances\n",
    "}).sort_values(by=\"ale_variance\", ascending=False)\n",
    "\n",
    "# Combine ALE scores for better robustness\n",
    "feature_importance_ale[\"combined_ale_score\"] = feature_importance_ale[\"ale_variance\"] + feature_importance_ale[\"ale_smooth_variance\"]\n",
    "\n",
    "# Compute Mutual Information\n",
    "mi_scores = mutual_info_classif(X_train_val_scaled, y_train_val)\n",
    "feature_importance_mi = pd.DataFrame({\n",
    "    \"feature\": X_train_val.columns,\n",
    "    \"mutual_info\": mi_scores\n",
    "}).sort_values(by=\"mutual_info\", ascending=False)\n",
    "\n",
    "# Combine ALE and Mutual Information scores\n",
    "combined_scores = feature_importance_ale.merge(\n",
    "    feature_importance_mi, on=\"feature\"\n",
    ").assign(combined_score=lambda df: df[\"combined_ale_score\"] * df[\"mutual_info\"])\n",
    "\n",
    "# Select top features based on combined score\n",
    "top_features = combined_scores.sort_values(by=\"combined_score\", ascending=False)[\"feature\"].iloc[:3]  # Increased to top 5\n",
    "X_train_val_selected = X_train_val_scaled[top_features]\n",
    "X_test_selected = X_test_scaled[top_features]\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree with expanded parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=1), param_grid=param_grid, cv=10, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_val_selected, y_train_val)\n",
    "\n",
    "# Best Decision Tree model\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_dt_model.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Final Accuracy with selected features and tuned Decision Tree: {accuracy:.4f}\")\n",
    "\n",
    "# Display top features based on combined score\n",
    "print(\"\\nTop features based on ALE and Mutual Information:\")\n",
    "print(combined_scores.sort_values(by=\"combined_score\", ascending=False).head(10))\n",
    "\n",
    "# Save combined feature importance for future analysis\n",
    "combined_scores.to_csv(\"feature_importance_analysis.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

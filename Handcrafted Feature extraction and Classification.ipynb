{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Paths\n",
    "dataset_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\dataset\"\n",
    "output_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\featuresHOG\"\n",
    "\n",
    "# Create output path if it does not exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {'benign': 0, 'malignant': 1}  # All lowercase for matching\n",
    "\n",
    "# Function to preprocess images and handle different formats\n",
    "def preprocess_image(image, target_size=(128, 128)):\n",
    "    if len(image.shape) == 3:  # RGB or RGBA image\n",
    "        if image.shape[2] == 4:  # RGBA image\n",
    "            image = image[:, :, :3]  # Discard the alpha channel\n",
    "        image = rgb2gray(image)  # Convert to grayscale\n",
    "    elif len(image.shape) != 2:  # Invalid shape\n",
    "        raise ValueError(\"Image must be 2D (grayscale) or 3D (RGB/RGBA)\")\n",
    "\n",
    "    # Resize to smaller size to save memory\n",
    "    image = resize(image, target_size, anti_aliasing=True)\n",
    "    return image\n",
    "\n",
    "# Function to calculate HOG features\n",
    "def calculate_hog_features(image):\n",
    "    return hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9, feature_vector=True)\n",
    "\n",
    "# Process dataset and extract HOG features\n",
    "def process_hog_features(dataset_path):\n",
    "    data = []\n",
    "    temp_file_path = os.path.join(output_path, 'features_temp.csv')\n",
    "    \n",
    "    # If the temporary file exists, remove it to avoid conflicts\n",
    "    if os.path.exists(temp_file_path):\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        # Map the label to its corresponding numeric value\n",
    "        label_mapped = label_mapping.get(label.lower(), -1)  # Convert to lowercase for comparison\n",
    "        if label_mapped == -1:\n",
    "            print(f\"Skipping folder with unexpected label: {label}\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            if not file_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = imread(file_path)\n",
    "                grayscale = preprocess_image(image)\n",
    "                hog_features = calculate_hog_features(grayscale)\n",
    "\n",
    "                features = {f'hog_{i}': val for i, val in enumerate(hog_features)}\n",
    "                features['label'] = label_mapped  # Add the label here\n",
    "                data.append(features)\n",
    "                \n",
    "                # Save features incrementally to avoid holding everything in memory\n",
    "                if len(data) >= 1000:  # Save every 1000 images\n",
    "                    temp_df = pd.DataFrame(data)\n",
    "                    temp_df.to_csv(temp_file_path, mode='a', header=not os.path.exists(temp_file_path), index=False)\n",
    "                    data = []  # Reset the data list\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {file_name} due to error: {e}\")\n",
    "\n",
    "    # Save remaining data after loop\n",
    "    if data:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        temp_df.to_csv(temp_file_path, mode='a', header=not os.path.exists(temp_file_path), index=False)\n",
    "\n",
    "    # Return the DataFrame with all extracted features\n",
    "    if os.path.exists(temp_file_path):\n",
    "        return pd.read_csv(temp_file_path)\n",
    "    else:\n",
    "        print(\"No features were extracted. Please check the dataset.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Extract features and split dataset\n",
    "features_df = process_hog_features(dataset_path)\n",
    "if features_df.empty:\n",
    "    print(\"No features were extracted. Please check the dataset.\")\n",
    "else:\n",
    "    # Ensure 'label' is present and split the dataset correctly\n",
    "    if 'label' not in features_df.columns:\n",
    "        print(\"Error: 'label' column not found in the features dataframe.\")\n",
    "    else:\n",
    "        X = features_df.drop('label', axis=1)\n",
    "        y = features_df['label']\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, stratify=y, random_state=1)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=1)\n",
    "\n",
    "        # Save features\n",
    "        pd.concat([X_train, y_train], axis=1).to_csv(os.path.join(output_path, 'train.csv'), index=False)\n",
    "        pd.concat([X_val, y_val], axis=1).to_csv(os.path.join(output_path, 'val.csv'), index=False)\n",
    "        pd.concat([X_test, y_test], axis=1).to_csv(os.path.join(output_path, 'test.csv'), index=False)\n",
    "\n",
    "        print(f\"Features saved to {output_path}\")\n",
    "        print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of HOG features using decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the combined feature files\n",
    "combined_features_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\featuresHOG\"\n",
    "\n",
    "# Load the combined train, val, and test CSV files\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test.csv\"))\n",
    "\n",
    "# Combine the train and val datasets\n",
    "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Split the features (X) and labels (y) for the combined training and validation set\n",
    "X_train_val = combined_train_val_df.drop('label', axis=1)  # Features\n",
    "y_train_val = combined_train_val_df['label']  # Labels\n",
    "\n",
    "# Split the features (X) and labels (y) for the test set\n",
    "X_test = test_df.drop('label', axis=1)  # Features\n",
    "y_test = test_df['label']  # Labels\n",
    "\n",
    "# ---- Grid Search for Hyperparameter Optimization ----\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Optimized Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best estimator from Grid Search\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# ---- Evaluate the Optimized Decision Tree ----\n",
    "# Training Evaluation\n",
    "y_train_pred = best_dt_classifier.predict(X_train_val)\n",
    "\n",
    "# Training Confusion Matrix\n",
    "train_cm = confusion_matrix(y_train_val, y_train_pred)\n",
    "print(\"\\nTraining Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Plot Training Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Training Classification Report\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train_val, y_train_pred, digits=4))\n",
    "\n",
    "# Training Accuracy\n",
    "train_accuracy = accuracy_score(y_train_val, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Testing Evaluation\n",
    "y_test_pred = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Testing Confusion Matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nTesting Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "# Plot Testing Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Testing Classification Report\n",
    "print(\"\\nTesting Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Testing Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HU moments features extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import moments_hu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "dataset_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\dataset\"\n",
    "output_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\HuMoments\"\n",
    "\n",
    "# Label mapping (ensure folder names match exactly as in the dataset)\n",
    "label_mapping = {'benign': 0, 'malignant': 1}  # All lowercase for matching\n",
    "\n",
    "# Function to preprocess images and handle different formats\n",
    "def preprocess_image(image):\n",
    "    if len(image.shape) == 3:  # RGB or RGBA image\n",
    "        if image.shape[2] == 4:  # RGBA image\n",
    "            image = image[:, :, :3]  # Discard the alpha channel\n",
    "        image = rgb2gray(image)  # Convert to grayscale\n",
    "    elif len(image.shape) != 2:  # Invalid shape\n",
    "        raise ValueError(\"Image must be 2D (grayscale) or 3D (RGB/RGBA)\")\n",
    "    return image\n",
    "\n",
    "# Function to calculate Hu moments\n",
    "def calculate_hu_moments(image):\n",
    "    return moments_hu(image)\n",
    "\n",
    "# Process dataset and extract Hu moments\n",
    "def process_hu_features(dataset_path):\n",
    "    data = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        label_mapped = label_mapping.get(label, -1)  # Use exact folder name match\n",
    "        if label_mapped == -1:\n",
    "            print(f\"Skipping folder with unexpected label: {label}\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            if not file_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = imread(file_path)\n",
    "                grayscale = preprocess_image(image)\n",
    "                hu_features = calculate_hu_moments(grayscale)\n",
    "\n",
    "                features = {f'hu_moment_{i}': val for i, val in enumerate(hu_features)}\n",
    "                features['label'] = label_mapped\n",
    "                data.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {file_name} due to error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract features and split dataset\n",
    "features_df = process_hu_features(dataset_path)\n",
    "if features_df.empty:\n",
    "    print(\"No features were extracted. Please check the dataset.\")\n",
    "else:\n",
    "    X = features_df.drop('label', axis=1)\n",
    "    y = features_df['label']\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, stratify=y, random_state=1)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=1)\n",
    "\n",
    "    # Save features\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(os.path.join(output_path, 'train.csv'), index=False)\n",
    "    pd.concat([X_val, y_val], axis=1).to_csv(os.path.join(output_path, 'val.csv'), index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(os.path.join(output_path, 'test.csv'), index=False)\n",
    "\n",
    "    print(f\"Features saved to {output_path}\")\n",
    "    print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of Hu moments features using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the combined feature files\n",
    "combined_features_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\HuMoments\"\n",
    "\n",
    "# Load the combined train, val, and test CSV files\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test.csv\"))\n",
    "\n",
    "# Combine the train and val datasets\n",
    "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Split the features (X) and labels (y) for the combined training and validation set\n",
    "X_train_val = combined_train_val_df.drop('label', axis=1)  # Features\n",
    "y_train_val = combined_train_val_df['label']  # Labels\n",
    "\n",
    "# Split the features (X) and labels (y) for the test set\n",
    "X_test = test_df.drop('label', axis=1)  # Features\n",
    "y_test = test_df['label']  # Labels\n",
    "\n",
    "# ---- Grid Search for Hyperparameter Optimization ----\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Optimized Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best estimator from Grid Search\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# ---- Evaluate the Optimized Decision Tree ----\n",
    "# Training Evaluation\n",
    "y_train_pred = best_dt_classifier.predict(X_train_val)\n",
    "\n",
    "# Training Confusion Matrix\n",
    "train_cm = confusion_matrix(y_train_val, y_train_pred)\n",
    "print(\"\\nTraining Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Plot Training Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Training Classification Report\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train_val, y_train_pred, digits=4))\n",
    "\n",
    "# Training Accuracy\n",
    "train_accuracy = accuracy_score(y_train_val, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Testing Evaluation\n",
    "y_test_pred = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Testing Confusion Matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nTesting Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "# Plot Testing Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Testing Classification Report\n",
    "print(\"\\nTesting Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Testing Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color histogram features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2hsv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "dataset_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\dataset\"\n",
    "output_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\ColorHistogram\"\n",
    "\n",
    "# Label mapping (ensure folder names match exactly as in the dataset)\n",
    "label_mapping = {'benign': 0, 'malignant': 1}  # All lowercase for matching\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image):\n",
    "    if len(image.shape) == 3:  # RGB or RGBA image\n",
    "        if image.shape[2] == 4:  # RGBA image\n",
    "            image = image[:, :, :3]  # Discard the alpha channel\n",
    "    elif len(image.shape) != 3 or image.shape[2] != 3:  # Invalid shape for RGB image\n",
    "        raise ValueError(\"Image must be RGB or RGBA format\")\n",
    "    return image\n",
    "\n",
    "# Function to calculate color histogram features\n",
    "def calculate_color_histogram(image, bins=10):\n",
    "    hsv_image = rgb2hsv(image)\n",
    "    features = {}\n",
    "    for i, channel in enumerate(['hue', 'saturation', 'value']):\n",
    "        hist, _ = np.histogram(hsv_image[:, :, i], bins=bins, range=(0, 1))\n",
    "        features[f'{channel}_mean'] = np.mean(hist)\n",
    "        features[f'{channel}_std'] = np.std(hist)\n",
    "    return features\n",
    "\n",
    "# Process dataset and extract color histogram features\n",
    "def process_color_hist_features(dataset_path, bins=10):\n",
    "    data = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        label_mapped = label_mapping.get(label, -1)  # Use exact folder name match\n",
    "        if label_mapped == -1:\n",
    "            print(f\"Skipping folder with unexpected label: {label}\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            if not file_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = imread(file_path)\n",
    "                image = preprocess_image(image)\n",
    "                features = calculate_color_histogram(image, bins=bins)\n",
    "                features['label'] = label_mapped\n",
    "                data.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {file_name} due to error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract features and split dataset\n",
    "features_df = process_color_hist_features(dataset_path, bins=10)\n",
    "if features_df.empty:\n",
    "    print(\"No features were extracted. Please check the dataset.\")\n",
    "else:\n",
    "    X = features_df.drop('label', axis=1)\n",
    "    y = features_df['label']\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, stratify=y, random_state=1)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=1)\n",
    "\n",
    "    # Save features\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(os.path.join(output_path, 'train.csv'), index=False)\n",
    "    pd.concat([X_val, y_val], axis=1).to_csv(os.path.join(output_path, 'val.csv'), index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(os.path.join(output_path, 'test.csv'), index=False)\n",
    "\n",
    "    print(f\"Features saved to {output_path}\")\n",
    "    print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of color histogram features using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the combined feature files\n",
    "combined_features_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\ColorHistogram\"\n",
    "\n",
    "# Load the combined train, val, and test CSV files\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test.csv\"))\n",
    "\n",
    "# Combine the train and val datasets\n",
    "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Split the features (X) and labels (y) for the combined training and validation set\n",
    "X_train_val = combined_train_val_df.drop('label', axis=1)  # Features\n",
    "y_train_val = combined_train_val_df['label']  # Labels\n",
    "\n",
    "# Split the features (X) and labels (y) for the test set\n",
    "X_test = test_df.drop('label', axis=1)  # Features\n",
    "y_test = test_df['label']  # Labels\n",
    "\n",
    "# ---- Grid Search for Hyperparameter Optimization ----\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Optimized Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best estimator from Grid Search\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# ---- Evaluate the Optimized Decision Tree ----\n",
    "# Training Evaluation\n",
    "y_train_pred = best_dt_classifier.predict(X_train_val)\n",
    "\n",
    "# Training Confusion Matrix\n",
    "train_cm = confusion_matrix(y_train_val, y_train_pred)\n",
    "print(\"\\nTraining Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Plot Training Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Training Classification Report\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train_val, y_train_pred, digits=4))\n",
    "\n",
    "# Training Accuracy\n",
    "train_accuracy = accuracy_score(y_train_val, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Testing Evaluation\n",
    "y_test_pred = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Testing Confusion Matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nTesting Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "# Plot Testing Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Testing Classification Report\n",
    "print(\"\\nTesting Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Testing Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USing training set to train, validation set to validate, and testing set to test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency-Domain Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fftpack import fft2, fftshift\n",
    "import pywt\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\dataset\"\n",
    "output_folder_base = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\FrequencyFeatures\"\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {'benign': 0, 'malignant': 1}  # All lowercase for matching\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image):\n",
    "    if len(image.shape) == 3:  # Convert RGB or RGBA to grayscale\n",
    "        if image.shape[2] == 4:  # If the image has an alpha channel (RGBA)\n",
    "            image = image[:, :, :3]  # Discard the alpha channel\n",
    "        image = rgb2gray(image)  # Convert to grayscale\n",
    "    return image\n",
    "\n",
    "# Function to calculate frequency features\n",
    "def calculate_frequency_features(image):\n",
    "    # Fourier Transform features\n",
    "    f_transform = fft2(image)\n",
    "    f_transform_shifted = fftshift(f_transform)\n",
    "    magnitude_spectrum = np.abs(f_transform_shifted)\n",
    "    norm_magnitude_spectrum = magnitude_spectrum / (np.sum(magnitude_spectrum) + 1e-6)\n",
    "\n",
    "    features = {\n",
    "        'peak_frequency': np.max(magnitude_spectrum),\n",
    "        'total_energy': np.sum(magnitude_spectrum ** 2),\n",
    "        'mean_frequency': np.mean(magnitude_spectrum),\n",
    "        'std_frequency': np.std(magnitude_spectrum),\n",
    "        'skew_frequency': skew(magnitude_spectrum.ravel()),\n",
    "        'kurtosis_frequency': kurtosis(magnitude_spectrum.ravel()),\n",
    "        'entropy_frequency': -np.sum(norm_magnitude_spectrum * np.log(norm_magnitude_spectrum + 1e-6)),\n",
    "        'bandwidth': np.sum(magnitude_spectrum > (0.5 * np.max(magnitude_spectrum))),\n",
    "    }\n",
    "\n",
    "    # Wavelet Transform features\n",
    "    coeffs = pywt.dwt2(image, 'haar')\n",
    "    cA, (cH, cV, cD) = coeffs\n",
    "    wavelet_features = {\n",
    "        'wavelet_energy': np.sum(cA ** 2),\n",
    "        'wavelet_entropy': -np.sum(cA / (np.sum(cA) + 1e-6) * np.log(cA / (np.sum(cA) + 1e-6) + 1e-6)),\n",
    "    }\n",
    "\n",
    "    features.update(wavelet_features)\n",
    "    return features\n",
    "\n",
    "# Function to extract and save frequency features\n",
    "def extract_and_save_frequency_features(dataset_path, output_folder):\n",
    "    data = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        label_mapped = label_mapping.get(label.lower(), -1)\n",
    "        if label_mapped == -1:\n",
    "            print(f\"Skipping unexpected label: {label}\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            if not file_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = imread(file_path)\n",
    "                image = preprocess_image(image)\n",
    "                features = calculate_frequency_features(image)\n",
    "                features['label'] = label_mapped\n",
    "                data.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No features extracted.\")\n",
    "        return\n",
    "\n",
    "    save_features_to_csv(data, output_folder, \"Frequency Features\")\n",
    "\n",
    "# Function to save features to CSV files\n",
    "def save_features_to_csv(data, output_folder, feature_type):\n",
    "    features_df = pd.DataFrame(data)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_df.drop('label', axis=1))\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=features_df.columns[:-1])\n",
    "    scaled_df['label'] = features_df['label']\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    train_df, temp_df = train_test_split(scaled_df, train_size=0.7, stratify=scaled_df['label'], random_state=1)\n",
    "    val_df, test_df = train_test_split(temp_df, train_size=0.5, stratify=temp_df['label'], random_state=1)\n",
    "\n",
    "    # Save datasets\n",
    "    feature_folder = os.path.join(output_folder, feature_type.replace(\" \", \"_\"))\n",
    "    os.makedirs(feature_folder, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(feature_folder, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(feature_folder, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(feature_folder, \"test.csv\"), index=False)\n",
    "\n",
    "    print(f\"Features saved to {feature_folder}\")\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Main execution\n",
    "output_folder = os.path.join(output_folder_base, \"Frequency_Features\")\n",
    "extract_and_save_frequency_features(dataset_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of frequency domain features using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the combined feature files\n",
    "combined_features_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\FrequencyFeatures\"\n",
    "\n",
    "# Load the combined train, val, and test CSV files\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test.csv\"))\n",
    "\n",
    "# Combine the train and val datasets\n",
    "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Split the features (X) and labels (y) for the combined training and validation set\n",
    "X_train_val = combined_train_val_df.drop('label', axis=1)  # Features\n",
    "y_train_val = combined_train_val_df['label']  # Labels\n",
    "\n",
    "# Split the features (X) and labels (y) for the test set\n",
    "X_test = test_df.drop('label', axis=1)  # Features\n",
    "y_test = test_df['label']  # Labels\n",
    "\n",
    "# ---- Grid Search for Hyperparameter Optimization ----\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Optimized Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best estimator from Grid Search\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# ---- Save Final Features ----\n",
    "final_train_val_path = os.path.join(combined_features_path, \"final_train_val.csv\")\n",
    "final_test_path = os.path.join(combined_features_path, \"final_test.csv\")\n",
    "\n",
    "# Save the training-validation and test features\n",
    "combined_train_val_df.to_csv(final_train_val_path, index=False)\n",
    "test_df.to_csv(final_test_path, index=False)\n",
    "\n",
    "print(f\"Final train-validation features saved to: {final_train_val_path}\")\n",
    "print(f\"Final test features saved to: {final_test_path}\")\n",
    "\n",
    "# ---- Evaluate the Optimized Decision Tree ----\n",
    "# Training Evaluation\n",
    "y_train_pred = best_dt_classifier.predict(X_train_val)\n",
    "\n",
    "# Training Confusion Matrix\n",
    "train_cm = confusion_matrix(y_train_val, y_train_pred)\n",
    "print(\"\\nTraining Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Plot Training Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Training Classification Report\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train_val, y_train_pred, digits=4))\n",
    "\n",
    "# Training Accuracy\n",
    "train_accuracy = accuracy_score(y_train_val, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Testing Evaluation\n",
    "y_test_pred = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Testing Confusion Matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nTesting Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "# Plot Testing Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Testing Classification Report\n",
    "print(\"\\nTesting Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Testing Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Binary Pattern (LBP) Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\dataset\"\n",
    "output_folder = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\LBP_Features\"\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {'benign': 0, 'malignant': 1}  # All lowercase for matching\n",
    "\n",
    "# Function to calculate LBP features\n",
    "def calculate_lbp_features(image):\n",
    "    # Local Binary Pattern (LBP) with P=8, R=1\n",
    "    lbp = local_binary_pattern(image, P=8, R=1, method='uniform')\n",
    "    # Calculate LBP histogram\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "    # Normalize the histogram\n",
    "    lbp_hist = lbp_hist.astype(np.float32)\n",
    "    lbp_hist /= (lbp_hist.sum() + 1e-6)\n",
    "    # Return features as a dictionary\n",
    "    return dict(zip([f'lbp_{i}' for i in range(len(lbp_hist))], lbp_hist))\n",
    "\n",
    "# Function to extract and save LBP features\n",
    "def extract_and_save_lbp_features(dataset_path, output_folder):\n",
    "    data = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Map label to corresponding numeric value\n",
    "        label_mapped = label_mapping.get(label.lower(), -1)\n",
    "        if label_mapped == -1:\n",
    "            print(f\"Skipping unexpected label: {label}\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            if not file_name.lower().endswith(('png', 'jpg', 'jpeg')):  # Ignore non-image files\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Read image\n",
    "                image = imread(file_path)\n",
    "                \n",
    "                # Handle RGBA images (4 channels)\n",
    "                if image.shape[-1] == 4:  \n",
    "                    image = image[:, :, :3]  # Drop the alpha channel (keep only RGB)\n",
    "                \n",
    "                # Convert to grayscale if the image is RGB\n",
    "                if len(image.shape) == 3 and image.shape[-1] == 3:  \n",
    "                    image = rgb2gray(image)\n",
    "                \n",
    "                # Validate that image is 2D (grayscale)\n",
    "                if len(image.shape) != 2:\n",
    "                    raise ValueError(f\"Invalid image shape {image.shape} for file: {file_name}\")\n",
    "                \n",
    "                # Extract LBP features\n",
    "                features = calculate_lbp_features(image)\n",
    "                # Add label to features\n",
    "                features['label'] = label_mapped\n",
    "                data.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No features extracted.\")\n",
    "        return\n",
    "\n",
    "    # Save features to CSV\n",
    "    save_features_to_csv(data, output_folder)\n",
    "\n",
    "# Function to save features to CSV\n",
    "def save_features_to_csv(data, output_folder):\n",
    "    # Convert data to a DataFrame\n",
    "    features_df = pd.DataFrame(data)\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_df.drop('label', axis=1))\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=features_df.columns[:-1])\n",
    "    scaled_df['label'] = features_df['label']\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_df, temp_df = train_test_split(scaled_df, train_size=0.7, shuffle=True, random_state=1, stratify=scaled_df['label'])\n",
    "    val_df, test_df = train_test_split(temp_df, train_size=0.5, shuffle=True, random_state=1, stratify=temp_df['label'])\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save CSV files directly to the specified folder\n",
    "    train_df.to_csv(os.path.join(output_folder, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(output_folder, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(output_folder, \"test.csv\"), index=False)\n",
    "\n",
    "    print(f\"Features saved to {output_folder}\")\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Extract and save LBP features\n",
    "extract_and_save_lbp_features(dataset_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of LBP using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the combined feature files\n",
    "combined_features_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\LBP_Features\"\n",
    "\n",
    "# Load the combined train, val, and test CSV files\n",
    "train_df = pd.read_csv(os.path.join(combined_features_path, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(combined_features_path, \"val.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(combined_features_path, \"test.csv\"))\n",
    "\n",
    "# Combine the train and val datasets\n",
    "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Split the features (X) and labels (y) for the combined training and validation set\n",
    "X_train_val = combined_train_val_df.drop('label', axis=1)  # Features\n",
    "y_train_val = combined_train_val_df['label']  # Labels\n",
    "\n",
    "# Split the features (X) and labels (y) for the test set\n",
    "X_test = test_df.drop('label', axis=1)  # Features\n",
    "y_test = test_df['label']  # Labels\n",
    "\n",
    "# ---- Grid Search for Hyperparameter Optimization ----\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Optimized Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best estimator from Grid Search\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# ---- Evaluate the Optimized Decision Tree ----\n",
    "# Training Evaluation\n",
    "y_train_pred = best_dt_classifier.predict(X_train_val)\n",
    "\n",
    "# Training Confusion Matrix\n",
    "train_cm = confusion_matrix(y_train_val, y_train_pred)\n",
    "print(\"\\nTraining Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Plot Training Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Training Classification Report\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train_val, y_train_pred, digits=4))\n",
    "\n",
    "# Training Accuracy\n",
    "train_accuracy = accuracy_score(y_train_val, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Testing Evaluation\n",
    "y_test_pred = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Testing Confusion Matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nTesting Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "# Plot Testing Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Testing Classification Report\n",
    "print(\"\\nTesting Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Testing Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabor Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.filters import gabor\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\dataset\"\n",
    "output_folder = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\GaborFeatures\"\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {'benign': 0, 'malignant': 1}  # All lowercase for matching\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image):\n",
    "    if len(image.shape) == 3:  # If the image has 3 channels (RGB or RGBA)\n",
    "        if image.shape[2] == 4:  # If the image has 4 channels (RGBA)\n",
    "            image = image[:, :, :3]  # Discard the alpha channel and keep RGB channels\n",
    "        image = rgb2gray(image)  # Convert to grayscale\n",
    "    return image\n",
    "\n",
    "# Function to calculate Gabor features\n",
    "def calculate_gabor_features(image):\n",
    "    frequencies = [0.1, 0.3, 0.5]\n",
    "    orientations = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    gabor_features = {}\n",
    "\n",
    "    for freq in frequencies:\n",
    "        for angle in orientations:\n",
    "            # Apply Gabor filter\n",
    "            real, _ = gabor(image, frequency=freq, theta=angle)\n",
    "            # Extract statistical features\n",
    "            gabor_features[f'gabor_{freq:.2f}_{angle:.2f}_mean'] = np.mean(real)\n",
    "            gabor_features[f'gabor_{freq:.2f}_{angle:.2f}_std'] = np.std(real)\n",
    "\n",
    "    return gabor_features\n",
    "\n",
    "# Function to extract and save Gabor features\n",
    "def extract_and_save_gabor_features(dataset_path, output_folder):\n",
    "    data = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        # Map label to numeric value\n",
    "        label_mapped = label_mapping.get(label.lower(), -1)\n",
    "        if label_mapped == -1:\n",
    "            print(f\"Skipping unexpected label: {label}\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            try:\n",
    "                # Read image\n",
    "                image = imread(file_path)\n",
    "                # Preprocess the image (convert to grayscale if RGB or RGBA)\n",
    "                image = preprocess_image(image)\n",
    "                # Extract Gabor features\n",
    "                features = calculate_gabor_features(image)\n",
    "                # Add label to features\n",
    "                features['label'] = label_mapped\n",
    "                data.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No features extracted.\")\n",
    "        return\n",
    "\n",
    "    # Save features to CSV\n",
    "    save_features_to_csv(data, output_folder)\n",
    "\n",
    "# Function to save features to CSV\n",
    "def save_features_to_csv(data, output_folder):\n",
    "    # Convert data to DataFrame\n",
    "    features_df = pd.DataFrame(data)\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_df.drop('label', axis=1))\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=features_df.columns[:-1])\n",
    "    scaled_df['label'] = features_df['label']\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_df, temp_df = train_test_split(scaled_df, train_size=0.7, shuffle=True, random_state=1, stratify=scaled_df['label'])\n",
    "    val_df, test_df = train_test_split(temp_df, train_size=0.5, shuffle=True, random_state=1, stratify=temp_df['label'])\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save CSV files directly to the specified folder\n",
    "    train_df.to_csv(os.path.join(output_folder, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(output_folder, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(output_folder, \"test.csv\"), index=False)\n",
    "\n",
    "    print(f\"Features saved to {output_folder}\")\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Extract and save Gabor features\n",
    "extract_and_save_gabor_features(dataset_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using gabor features using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the combined feature files\n",
    "combined_features_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\GaborFeatures\"\n",
    "\n",
    "# Load train, val, and test datasets\n",
    "try:\n",
    "    train_df = pd.read_csv(os.path.join(combined_features_path, \"train.csv\"))\n",
    "    val_df = pd.read_csv(os.path.join(combined_features_path, \"val.csv\"))\n",
    "    test_df = pd.read_csv(os.path.join(combined_features_path, \"test.csv\"))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Combine the train and val datasets\n",
    "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Ensure label column exists in both combined train-val and test datasets\n",
    "if 'label' not in combined_train_val_df.columns or 'label' not in test_df.columns:\n",
    "    raise ValueError(\"The 'label' column is missing in one of the datasets.\")\n",
    "\n",
    "# Split the features (X) and labels (y) for the training-validation set\n",
    "X_train_val = combined_train_val_df.drop(columns=['label'])  # Features\n",
    "y_train_val = combined_train_val_df['label']  # Labels\n",
    "\n",
    "# Split the features (X) and labels (y) for the test set\n",
    "X_test = test_df.drop(columns=['label'])  # Features\n",
    "y_test = test_df['label']  # Labels\n",
    "\n",
    "# Check for any missing values in the datasets\n",
    "if X_train_val.isnull().values.any() or X_test.isnull().values.any():\n",
    "    print(\"Warning: Missing values found in features. Please handle missing data before training.\")\n",
    "\n",
    "# ---- Grid Search for Hyperparameter Optimization ----\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Optimized Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best estimator from Grid Search\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# ---- Training Evaluation ----\n",
    "# Make predictions on the training data\n",
    "y_train_pred = best_dt_classifier.predict(X_train_val)\n",
    "\n",
    "# Training confusion matrix\n",
    "train_cm = confusion_matrix(y_train_val, y_train_pred)\n",
    "print(\"Training Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Plot training confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=[f'Class {i}' for i in range(len(set(y_train_val)))],\n",
    "            yticklabels=[f'Class {i}' for i in range(len(set(y_train_val)))])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Training classification report\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train_val, y_train_pred, digits=4))\n",
    "\n",
    "# Training accuracy\n",
    "train_accuracy = accuracy_score(y_train_val, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# ---- Testing Evaluation ----\n",
    "# Make predictions on the test data\n",
    "y_test_pred = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Testing confusion matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Testing Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "# Plot testing confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Class {i}' for i in range(len(set(y_test)))],\n",
    "            yticklabels=[f'Class {i}' for i in range(len(set(y_test)))])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Testing classification report\n",
    "print(\"\\nTesting Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Testing accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

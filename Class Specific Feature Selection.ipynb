{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6a6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f2aa4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f36a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Performing feature selection and training for class 0...\n",
      "âœ… Selected features for class 0: ['mean_frequency', 'gabor_0.10_1.57_std', 'gabor_0.30_1.57_std', 'gabor_0.50_1.57_std', 'hog_805']\n",
      "ğŸ“ˆ Accuracy for class 0: 0.7160\n",
      "ğŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7196    0.6937    0.7064       666\n",
      "           1     0.7127    0.7376    0.7249       686\n",
      "\n",
      "    accuracy                         0.7160      1352\n",
      "   macro avg     0.7162    0.7157    0.7157      1352\n",
      "weighted avg     0.7161    0.7160    0.7158      1352\n",
      "\n",
      "\n",
      "ğŸ” Performing feature selection and training for class 1...\n",
      "âœ… Selected features for class 1: ['mean_frequency', 'gabor_0.10_1.57_std', 'gabor_0.30_1.57_std', 'gabor_0.50_1.57_std', 'hog_805']\n",
      "ğŸ“ˆ Accuracy for class 1: 0.7204\n",
      "ğŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7151    0.7464    0.7304       686\n",
      "           1     0.7264    0.6937    0.7097       666\n",
      "\n",
      "    accuracy                         0.7204      1352\n",
      "   macro avg     0.7207    0.7200    0.7200      1352\n",
      "weighted avg     0.7207    0.7204    0.7202      1352\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ğŸ“˜ Step 0: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ğŸ“˜ Step 1: Load Feature Files\n",
    "def load_feature_files(base_path):\n",
    "    \"\"\"Load train, validation, and test CSV files from each feature folder.\"\"\"\n",
    "    feature_sets = [\"Frequency\", \"Gabor\", \"HOG\", \"Statistical\"]\n",
    "    data = {}\n",
    "\n",
    "    for feature in feature_sets:\n",
    "        train_path = os.path.join(base_path, feature, \"train.csv\")\n",
    "        val_path = os.path.join(base_path, feature, \"val.csv\")\n",
    "        test_path = os.path.join(base_path, feature, \"test.csv\")\n",
    "\n",
    "        data[feature] = {\n",
    "            \"train\": pd.read_csv(train_path),\n",
    "            \"val\": pd.read_csv(val_path),\n",
    "            \"test\": pd.read_csv(test_path),\n",
    "        }\n",
    "\n",
    "    return data\n",
    "\n",
    "base_path = r\"E:\\Abroad period research\\Feature Fusion paper\\Ultrasound Breast Cancer\\Features\"\n",
    "data = load_feature_files(base_path)\n",
    "\n",
    "# ğŸ“˜ Step 2: Split Features and Labels\n",
    "def split_features_and_labels(data):\n",
    "    \"\"\"Split features and labels for each feature set.\"\"\"\n",
    "    X_train, y_train = {}, {}\n",
    "    X_val, y_val = {}, {}\n",
    "\n",
    "    for feature, datasets in data.items():\n",
    "        X_train[feature] = datasets[\"train\"].iloc[:, :-1]\n",
    "        y_train[feature] = datasets[\"train\"].iloc[:, -1]\n",
    "        X_val[feature] = datasets[\"val\"].iloc[:, :-1]\n",
    "        y_val[feature] = datasets[\"val\"].iloc[:, -1]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "X_train, y_train, X_val, y_val = split_features_and_labels(data)\n",
    "\n",
    "# ğŸ“˜ Step 3: Class-Specific Feature Selection and Evaluation\n",
    "def perform_class_specific_feature_selection(X_train, y_train, X_val, y_val, num_features=5):\n",
    "    \"\"\"\n",
    "    Perform one-vs-all feature selection and train a Decision Tree for each class.\n",
    "    \"\"\"\n",
    "    class_specific_features = {}\n",
    "    class_classifiers = {}\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Use labels from any one feature set (assuming they're the same)\n",
    "    classes = np.unique(y_train[next(iter(y_train))])\n",
    "\n",
    "    for class_label in classes:\n",
    "        print(f\"\\nğŸ” Performing feature selection and training for class {class_label}...\")\n",
    "\n",
    "        # One-vs-all labels\n",
    "        y_binary_train = y_train[next(iter(X_train))].apply(lambda x: 1 if x == class_label else 0)\n",
    "        y_binary_val = y_val[next(iter(X_val))].apply(lambda x: 1 if x == class_label else 0)\n",
    "\n",
    "        # Merge all feature sets\n",
    "        X_train_combined = pd.concat([X_train[feature] for feature in X_train], axis=1)\n",
    "        X_val_combined = pd.concat([X_val[feature] for feature in X_val], axis=1)\n",
    "\n",
    "        # Handle missing values (NaNs) using SimpleImputer\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        X_train_imputed = imputer.fit_transform(X_train_combined)\n",
    "        X_val_imputed = imputer.transform(X_val_combined)\n",
    "\n",
    "        # Select top-k features\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n",
    "        selector.fit(X_train_imputed, y_binary_train)\n",
    "\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features = X_train_combined.columns[selected_indices]\n",
    "\n",
    "        class_specific_features[class_label] = selected_features\n",
    "        print(f\"âœ… Selected features for class {class_label}: {list(selected_features)}\")\n",
    "\n",
    "        # Train model\n",
    "        clf = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        clf.fit(X_train_imputed[:, selected_indices], y_binary_train)\n",
    "\n",
    "        # Evaluate model\n",
    "        y_pred_val = clf.predict(X_val_imputed[:, selected_indices])\n",
    "        accuracy = accuracy_score(y_binary_val, y_pred_val)\n",
    "        report = classification_report(y_binary_val, y_pred_val, digits=4)\n",
    "\n",
    "        class_classifiers[class_label] = clf\n",
    "        evaluation_results[class_label] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "\n",
    "        print(f\"ğŸ“ˆ Accuracy for class {class_label}: {accuracy:.4f}\")\n",
    "        print(f\"ğŸ“‹ Classification Report:\\n{report}\")\n",
    "\n",
    "    return class_specific_features, class_classifiers, evaluation_results\n",
    "\n",
    "# ğŸ“˜ Step 4: Run Feature Selection + Training + Evaluation\n",
    "class_specific_features, class_classifiers, evaluation_results = perform_class_specific_feature_selection(\n",
    "    X_train, y_train, X_val, y_val, num_features=5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
